# 分布式

## CAP理论
CAP 理论对分布式系统的特性做了高度抽象，形成了三个指标
- Consistency

  一致性说的是客户端的每次读操作，不管访问哪个节点，要么读到的都是同一份最新写入的数据，要么读取失败

- Availability

   可用性说的是任何来自客户端的请求，不管访问哪个非故障节点，都能得到响应数据，但不保证是同一份最新数据

- Partition Tolerance

   当节点间出现任意数量的消息丢失或高延迟的时候，系统仍然在继续工作。也就是说，分布式系统在告诉访问本系统的客户端：不管我的内部出现什么样的数据同步问题，我会一直运行。这个指标，强调的是集群对分区故障的容错能力
   
   因为分布式系统与单机系统不同，它涉及到多节点间的通讯和交互，节点间的分区故障是必然发生的，在分布式系统中分区容错性是必须要考虑的

大部分人对CAP理论有个误解，认为无论在什么情况下，分布式系统都只能在C和A中选择1个。其实，在不存在网络分区的情况下，也就是分布式系统正常运行时（这也是系统在绝大部分时候所处的状态），就是说在不需要 P 时，C 和 A 能够同时保证。只有当发生分区故障的时候，也就是说需要 P 时，才会在 C 和 A 之间做出选择。而且如果读操作会读到旧数据，影响到了系统运行或业务运行（也就是说会有负面的影响），推荐选择 C，否则选 A

- 如何使用CAP

只要有网络交互就一定会有延迟和数据丢失，而这种状况我们必须接受，还必须保证系统不能挂掉。所以就像我上面提到的，节点间的分区故障是必然发生的。也就是说，分区容错性（P）是前提，是必须要保证的

- 样例

InfluxDB是由META节点和DATA节点2个逻辑单元组成，这2个节点的功能和数据特点不同，需要我们分别为它们设计分区容错一致性模型

1. 考虑到META节点保存的是系统运行的关键元信息，**采用CP架构**
2. DATA节点保存的是具体的时序数据记录，服务会被访问频繁，水平扩展、性能、可用性等是关键，**采用AP架构**

- CP模型：典型的应用是 Etcd，Consul和Hbase
- AP模型：典型应用就比如Cassandra和DynamoDB


## Paxos算法

### Basic Paxos算法

**描述的是多节点之间如何就某个值达成共识**

使用了比较重要的概念：提案、准备（Prepare）请求、接受（Accept）请求、角色

![](basicPaxos.png)

1. 角色
   - 提议者（Proposer）: 代表的是接入和协调功能，收到客户端请求后，发起二阶段提交，进行共识协商
   - 接受者（Acceptor）: 接受者代表投票协商和存储数据，对提议的值进行投票，并接受达成共识的值，存储保存
   - 学习者（Learner）
2. 提案
3. 准备（Prepare）请求
    - 提案编号，编号越大的优先级越高
4. 接受（Accept）请求
    - 就是需要达成共识的value投票

Basic Paxos的**容错能力**，源自“大多数”的约定，你可以这么理解：当少于一半的节点出现故障的时候，共识协商仍然在正常工作

- 思考:

如果节点 A、B 已经通过了提案[5, 7]，节点 C 未通过任何提案，那么当客户端 3 提案编号为 9 时，通过 Basic Paxos 执行“SET X = 6”，最终三个节点上 X 值是多少呢？

如果节点 A、B 已经通过了提案[5, 7]，节点 C 未通过任何提案，那么当客户端 3 提案编号为 9 ，通过 Basic Paxos 执行“SET X = 6”, 最终节点值应该是[9,7], 过程如下：
1. 在准备阶段，节点C收到客户端3的准备请求[9,6], 因为节点C未收到任何提案，所以返回“尚无提案”的相应。这时如果节点C收到了之前客户端的准备请求[5, 7], 根据提案编号5小于它之前响应的准备请求的提案编号9，会丢弃该准备请求。
2. 客户端3发送准备请求[9,6]给节点A，B，这时因为节点A，B已经通过了提案[5,7], 根据“如果接受者之前有通过提案，那么接受者将承诺，会在准备请求的响应中，包含已经通过的最大编号的提案信息”，节点A，B会返回[5,7]给客户端3.
3. 客户端3发送会接受请求[9,7]给节点A，B，C（注意这里使用的是准备阶段的最大提议编号和已经被通过的值），因为此时请求编号9不小于之前的请求编号，所以所有节点接受该请求[9,7].
4. 所有学习者会接受该提案，学习该值

**对一个key如果达成共识，后面该key的值也不会变了**


### Multi Paxos算法
   Multi-Paxos 算法是一个统称，它是指基于 Multi-Paxos 思想，通过多个 Basic Paxos 实例实现一系列值的共识的算法（比如 Chubby 的 Multi-Paxos 实现、Raft 算法等）

Basic Paxos是通过二阶段提交来达成共识的。在第一阶段，也就是准备阶段，接收到大多数准备响应的提议者，才能发起接受请求进入第二阶段（也就是接受阶段）

Basic Paxos的问题：

1. 如果多个提议者同时提交提案，可能出现因为提案编号冲突，在准备阶段没有提议者接收到大多数准备响应，协商失败，需要重新协商。一个5节点的集群，如果3个节点作为提议者同时提案，就可能发生因为没有提议者接收大多数响应（比如1个提议者接收到1个准备响应，另外2个提议者分别接收到2个准备响应）而准备失败，需要重新协商
2. 2轮RPC 通讯（准备阶段和接受阶段）往返消息多、耗性能、延迟大

## Quorum NWR算法

- N: 表示副本数，又叫做复制因子（Replication Factor）
- W，又称写一致性级别（Write Consistency Level），表示成功完成 W 个副本更新，才完成写操作
- R，又称读一致性级别（Read Consistency Level）


## DDIA

### Partition

分片通常和复制结合使用。每个分片有多个副本，可以分散到多机上去（更泛化一点：多个容错阈）；同时，每个机器含有多个分片，但通常不会有一个分片的两个副本放到一个机器上。

![](Partition-1.png)

Partition的本质是对数据集合的划分。但在实践中，可以细分为两个步骤：

1. 对数据集进行逻辑划分

    分片过程中，要保证每个分片的数据量多少尽量均匀，否则会有数据偏斜（skew），甚而形成数据热点。

2. 将逻辑分片调度到物理节点

   分片后，需要保存路由信息，给一个KV条目，能知道去哪个机器上去查；稍差一些，可以知道去哪几个机器上去找；最差的，如果需要去所有机器逐一查询，但性能一般不可接受。

#### 对数据集进行逻辑划分

- 按键范围分区

HBase采用了一种自动去“分裂”（split）的方式来动态地进行分区

![](Bigtable-2.png)

整个数据表，会按照行键排好序，然后按照连续的行键一段段地分区。如果某一段行键的区间里，写的数据越来越多，占用的存储空间越来越大，那么整个系统会自动地将这个分区一分为二，变成两个分区。而如果某一个区间段的数据被删掉了很多，占用的空间越来越小了，那么我们就会自动把这个分区和它旁边的分区合并到一起

- 键散列（Hash）分区

优势：避免了于偏斜和热点的风险

劣势：失去了键范围分区的高效执行范围查询的能力

- 一个折中的策略是一个列会作为散列的依据，而其他的按照顺序存储在单个分区上

   Example:
   
   Cassandra中的表可以使用由多个列组成的复合主键来声明。键中只有第一列会作为散列的依据，而其他列则被用SSTables中排序数据的连接索引
   
   社交媒体网站上，一个用户可能会发布很多更新。如果更新的主键被选择为 (user_id, update_timestamp) ，那么可以有效地检索特定用户在某个时间间隔内按时间戳排序的所有更新。不同的用户可以存储在 不同的分区上，对于每个用户，更新按时间戳顺序存储在单个分区上
   
   kafka和ES均采用这种方式对数据分区

- 开源组件代码实现
1. ES

    插入的逻辑在IndexRequest#doRun，首先会对插入文档的id字段赋值
    ```Java
       // generate id if not already provided
            if (id == null) {
                String uid = UUIDs.base64UUID();
                id(uid);
            }
    ```

    然后再计算分到哪个shard
    
    ```Java
    ShardId shardId = clusterService.operationRouting().indexShards(clusterState, concreteIndex, request.id(),
    request.routing()).shardId();
    ```
    
    具体的计算如下：

    ```Java
     public static int generateShardId(IndexMetadata indexMetadata, @Nullable String id, @Nullable String routing) {
            if (routing == null) {
                assert(indexMetadata.isRoutingPartitionedIndex() == false) : "A routing value is required for gets from a partitioned index";
                effectiveRouting = id;
            } else {
                effectiveRouting = routing;
            }
    
            if (indexMetadata.isRoutingPartitionedIndex()) {
                // hash(id) % routing_partition_size
                partitionOffset = Math.floorMod(Murmur3HashFunction.hash(id), indexMetadata.getRoutingPartitionSize());
            } else {
                // we would have still got 0 above but this check just saves us an unnecessary hash calculation
                partitionOffset = 0;
            }
    
            return calculateScaledShardId(indexMetadata, effectiveRouting, partitionOffset);
        }
    
        private static int calculateScaledShardId(IndexMetadata indexMetadata, String effectiveRouting, int partitionOffset) {
            final int hash = Murmur3HashFunction.hash(effectiveRouting) + partitionOffset;
    
            // we don't use IMD#getNumberOfShards since the index might have been shrunk such that we need to use the size
            // of original index to hash documents
            // indexMetadata.getRoutingNumShards只包含主分片
            return Math.floorMod(hash, indexMetadata.getRoutingNumShards()) / indexMetadata.getRoutingFactor();
        }
    ```

    indexMetadata.getRoutingPartitionSize()配置来减少倾斜的风险。routing_partition_size越大，数据的分布越均匀

    最后计算公式可以合并为：
    <code-block lang="tex"> shardNum=(hash(routing)+hash(id) \% routingPartitionSize) \% numPrimaryShards </code-block>

2. kafka

   KIP-480: Sticky Partitioner引入了UniformStickyPartitioner作为默认的分区器。这个是在Round-robin策略上的优化

   会从本地缓存中拿对应topic的分区，所以具有粘性(Sticky),只有当newBatch或者indexCache为空的情况下才会重新计算分区
    ```Java
       public int partition(String topic, Cluster cluster) {
        Integer part = indexCache.get(topic);
        if (part == null) {
            return nextPartition(topic, cluster, -1);
        }
        return part;
    }
   ```

   newBatch指的是该batch已经满或者到达了发送的时间。UniformStickyPartitioner计算分区也很简单，即随机数

   ```Java
         if (availablePartitions.size() == 1) {
                newPart = availablePartitions.get(0).partition();
            } else {
                while (newPart == null || newPart.equals(oldPart)) {
                    int random = Utils.toPositive(ThreadLocalRandom.current().nextInt());
                    newPart = availablePartitions.get(random % availablePartitions.size()).partition();
                }
            }
   ```
   
   但是UniformStickyPartitioner有在某些场景下会有问题，在3.3.0废弃，[KIP-794](https://cwiki.apache.org/confluence/display/KAFKA/KIP-794%3A+Strictly+Uniform+Sticky+Partitioner)做了优化，解决**分配倾斜**

   UniformStickyPartitioner会将更多消息分配给速度较慢的broker，并且可能导致“失控”的问题。因为“粘性”时间是由新的批量创建消息驱动的，这与broker的延迟成反比——较慢的broker消耗批量消息的速度较慢，因此它们会比速度更快的分区获得更多的“粘性”时间，从而是消息分配倾斜。

   假设一个生产者写入的消息到3个分区（生产者配置为linger.ms=0），并且一个partition由于某种原因（leader broker选举更换或网络抖动问题等情况）导致稍微变慢了一点。生产者必须一直保留发送到这个partition的批次消息，直到partition变得可用。在保留这些批次消息的同时，因为生产者还没有准备好发送到这个分区，其他批次的消息又大批量发送的并开始堆积，从而可能导致每个批次都可能会被填满。

   KIP-794对UniformStickyPartitioner做了优化，可以采用自适应分区切换

   - 切换策略：分配分区的概率与队列长度成反比
   
   每次partitionReady之后，更新partition的统计信息
   ```Java
   topicInfo.builtInPartitioner.updatePartitionLoadStats(queueSizes, partitionIds, queueSizesIndex + 1);
    .....
   partitionLoadStats = new PartitionLoadStats(queueSizes, partitionIds, length);
   ```
   
   PartitionLoadStats的构造函数
   
      ```Java
        private final static class PartitionLoadStats {
           public final int[] cumulativeFrequencyTable; 
           public final int[] partitionIds;
           public final int length;
      ```
   
   主要的逻辑是cumulativeFrequencyTable的构造，注释中举了个例子
   
   ```
   Example: 
     假设有3个partitions的队列长度分别是:
     0 3 1
     最大的queue的长度+1则等于3+1=4，再减去每个queue的长度则是
     4 1 3
     再做前缀和，则cumulativeFrequencyTable数组可以赋值为
     4 5 8
   ```
   
   那构造了CFT数组如何去用呢，取一个随机数[0..8)，然后看它比CFT数组哪个值比较大则去对应下标。如随机到4的话，选择第二个Partition

#### 将逻辑分片调度到物理节点

xxxxx

#### 分片与次级索引

次级索引通常并不能唯一地标识记录，而是一种搜索记录中出现特定值的方式：查找用户123的所有操作，查找包含词语hogwash的所有文章，查找所有颜色为红色的车辆等等

> 书中说 “次级索引也是Solr和Elasticsearch等搜索服务器的基石”，
> 他指的次级索引可能指的是正排和倒排

- 文档的分区（document-based） 正排？
  ![](document-based.png)

- 基于关键词（term-based）的分区 倒排？


### 事务

   
